https://www.baeldung.com/ops/kafka-docker-setup
Run command from the directory -
docker-compose up -d


Check if Kafka and ZooKeeper working fine or not --
$ nc -z localhost 22181
Connection to localhost port 22181 [tcp/*] succeeded!
$ nc -z localhost 29092
Connection to localhost port 29092 [tcp/*] succeeded!
$ nc -z localhost 5432
Connection to localhost port 5432 [tcp/postgresql] succeeded!
$ nc -z localhost 5438
Connection to localhost port 5438 [tcp/*] succeeded!

Check Docker Logs for kafka ---
$ docker-compose logs kafka | grep -i started

pip install kafka-python
pip install confluent-kafka
pip install psycopg2

----
Install DBeaver (tool for visualizing database)
Connect to Postgres
Run this script to create employee table and CDC table in database 1
```
CREATE TABLE employees(
emp_id SERIAL,
first_name   VARCHAR(50),
last_name VARCHAR(50),
dob DATE,
city VARCHAR(40)
);

CREATE TABLE employees_cdc( 
emp_id SERIAL,
first_name VARCHAR(100), 
last_name VARCHAR(100), 
dob DATE,
city VARCHAR(100), 
action varchar(100) );
```

Run the script to create trigger function in database 1 to record change status accordingly
```
-- Create the trigger function
CREATE OR REPLACE FUNCTION employees_trigger_function()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO employees_cdc (first_name, last_name, dob, city, action)
        VALUES (NEW.first_name, NEW.last_name, NEW.dob, NEW.city, 'INSERT');
    ELSIF TG_OP = 'UPDATE' THEN
        INSERT INTO employees_cdc (first_name, last_name, dob, city, action)
        VALUES (NEW.first_name, NEW.last_name, NEW.dob, NEW.city, 'UPDATE');
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO employees_cdc (first_name, last_name, dob, city, action)
        VALUES (OLD.first_name, OLD.last_name, OLD.dob, OLD.city, 'DELETE');
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Create the trigger
CREATE TRIGGER employees_trigger
AFTER INSERT OR UPDATE OR DELETE ON employees
FOR EACH ROW EXECUTE FUNCTION employees_trigger_function(); 
```

Run this script to create employee table in database 2 to perform synchronized updates
```
CREATE TABLE employees( 
emp_id SERIAL,
first_name VARCHAR(100), 
last_name VARCHAR(100), 
dob DATE,
city VARCHAR(100) );
```

After creating table schema in both databases, need to setup connection with Kafka offset explorer
Run producer.py to broadcast changes in DB1 to Kafka (need to run every time after changes made)
Run consumer.py to stream and synchronize changes in DB2 from Kafka

To connect a SQL script, need to open it and connect database on the top of console from N/A to target DB

